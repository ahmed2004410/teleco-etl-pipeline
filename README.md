# ğŸš€ Enterprise Telecom Churn Data Warehouse Pipeline
**Built with Astro CLI | Apache Airflow | Docker | Medallion Architecture**

![Astro](https://img.shields.io/badge/Astro-CLI-purple?style=flat&logo=astronomer)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.9-blue?style=flat&logo=apache-airflow)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?style=flat&logo=docker)
![Postgres](https://img.shields.io/badge/Postgres-Data%20Warehouse-336791?style=flat&logo=postgresql)
![Status](https://img.shields.io/badge/Pipeline-Production%20Ready-green)

---

## ğŸ“– Executive Summary

This project is a production-grade **Data Engineering Solution** designed to process high-volume Telecom customer data. Unlike standard ETL pipelines, this system features a **Self-Healing Architecture** with automated Data Quality enforcement.

The pipeline is fully containerized using **Docker** and orchestrated via **Astro CLI (Airflow)**, implementing the **Medallion Architecture** (Bronze, Silver, Gold) to transform raw logs into analytical insights (Star Schema).

---

## ğŸŒŸ Key Features & Advanced Capabilities

### 1. ğŸ›¡ï¸ Automated Data Quality & Quarantine (The Circuit Breaker)
The pipeline does not just "fail" on bad data; it manages it intelligently:
* **Threshold-Based Validation:** If error rates exceed a defined threshold (e.g., 50 rows), the pipeline halts to prevent warehouse pollution.
* **Quarantine Logic:** Rows with specific issues (Negative Tenure, Invalid Gender, etc.) are **automatically isolated** from the clean batch.
* **Reporting:** The system generates an Excel report of rejected rows and emails it to the data steward immediately.

### 2. ğŸ”„ "LoopBack" Reprocessing Mechanism (Correction Pipeline)
I implemented a dedicated **Event-Driven DAG** (`churn_99_reprocessing`) to handle fixed data:
* **Smart Sensors:** Continuously watches for corrected files in the `fixed_data/` directory.
* **Idempotency:** Uses `Upsert` logic (Delete + Insert) to ensure no duplicate records when re-processing data.
* **Auto-Recovery:** Once data is fixed, it automatically promotes it to Silver and refreshes the Gold layer.

### 3. ğŸ—ï¸ Modern Infrastructure
* **Astro Framework:** Leveraging the modern way to run Airflow for better developer experience and deployment.
* **Dockerized Environment:** Ensures consistency across Development, Staging, and Production.
* **Modular SQL:** Transformation logic is decoupled from Python code, stored in organized `SQL/` directories for maintainability.

---

## âš™ï¸ Architecture & Data Flow

The project follows the **Medallion Architecture**:

| Layer | Component | Function | Technology |
| :--- | :--- | :--- | :--- |
| **Ingestion** | `load_csv_to_staging` | Detects new CSVs, creates Staging tables, archives raw files. | Python / Pandas |
| **ğŸ¥‰ Bronze** | `fill_bronze` | Raw data ingestion with initial tracking columns. | SQL / Postgres |
| **ğŸ¥ˆ Silver** | `clean_silver_task` | **Complex Cleaning:** Deduplication, Type Casting, Null Handling. Bad data is moved to `include/quarantine`. | Python / SQL |
| **ğŸ¥‡ Gold** | `fill_gold` | Business Logic Aggregation. Creates Fact & Dimension tables (**Star Schema**). | SQL (Data Marts) |

---

## ğŸ› ï¸ Tech Stack

* **Orchestration:** Apache Airflow (via Astro CLI)
* **Language:** Python 3.9 (Pandas, SQLAlchemy)
* **Database:** PostgreSQL (Local Data Warehouse)
* **Containerization:** Docker & Docker Compose
* **Alerting:** SMTP (Gmail Relay) for failure & quality alerts.
* **Testing:** `pytest` for DAG integrity & logic validation.

---

## ğŸ“‚ Project Structure

```text
TELECO-ETL-PIPELINE/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ DataWarehouse.py           # ğŸš€ Main Daily ETL Pipeline
â”‚   â”œâ”€â”€ Reprocessing.py            # ğŸ”„ Event-Driven Fix Pipeline
â”‚   â””â”€â”€ SQL/                       # Modular SQL Scripts
â”‚       â”œâ”€â”€ Bronze/                # Raw DDLs & Inserts
â”‚       â”œâ”€â”€ Silver/                # Cleaning Logic
â”‚       â””â”€â”€ Gold/                  # Fact/Dim Creation
â”‚
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ staging/                   # Landing zone for new files
â”‚   â”œâ”€â”€ quarantine/                # âš ï¸ Automated rejected data landing
â”‚   â”œâ”€â”€ fixed_data/                # ğŸ“¥ Drop zone for corrected files
â”‚   â””â”€â”€ archive/                   # Historical raw data
â”‚
â”œâ”€â”€ tests/                         # CI/CD Tests
â”‚   â””â”€â”€ test_dag_integrity.py      # Ensures no cyclic dependencies
â”‚
â”œâ”€â”€ Dockerfile                     # Astro Runtime Image
â”œâ”€â”€ packages.txt                   # OS dependencies
â””â”€â”€ requirements.txt               # Python libs (Pandas, Postgres, etc.)
```
---
##ğŸ”” Monitoring & Alerts
**âš ï¸ On Failure**

* **Instant email alert including:**

* **DAG ID**

* **Task ID**

* **Error Log (Stack Trace)**

* **ğŸš« On Data-Quality Rejection**

* **Auto-generated Excel Report**

* **Automatically emailed to the Operations Team**

* **Includes detailed reason for every rejected record
(e.g., Missing ID, Negative Tenure, Invalid Gender)**

---

##ğŸš€ How to Run

###1ï¸âƒ£ Clone & Start
git clone https://github.com/YourUsername/Telecom-ETL-Pipeline.git
cd Telecom-ETL-Pipeline
astro dev start

---

###2ï¸âƒ£ Access Airflow

Open your browser at:

ğŸ‘‰ http://localhost:8080

Login credentials:

Username: admin

Password: admin

---

###3ï¸âƒ£ Trigger the Pipeline

Place your source CSV file into:

include/staging/


Enable the DAG:

---

###â¡ï¸ Data_Warehouse_Full_Pipeline

Then simply sit back and watch the magic happen âœ¨

##ğŸ‘¨â€ğŸ’» Author

**Ahmed Anwer Fath**
Data Engineer
